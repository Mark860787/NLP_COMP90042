{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/duhaozhou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/duhaozhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/duhaozhou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/duhaozhou/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Exploratory Data Analysis (EDA session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a). load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_all_data(data_dir='data'):\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    train_claims = load_json_data(data_dir / 'train-claims.json')\n",
    "    dev_claims = load_json_data(data_dir / 'dev-claims.json')\n",
    "    test_claims = load_json_data(data_dir / 'test-claims-unlabelled.json')\n",
    "    evidences = load_json_data(data_dir / 'evidence.json')\n",
    "\n",
    "    return train_claims, dev_claims, test_claims, evidences\n",
    "train_data, dev_data, test_data, evidence_data = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b). Breif summary of train dataset & evidence dataset (Max /Min /Mean / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train claim count: 1228\n",
      "Max claim length: 332\n",
      "Min claim length: 26\n",
      "Mean claim length: 122.95521172638436\n",
      "Max evidence count per claim: 5\n",
      "Min evidence count per claim: 1\n",
      "Mean evidence count per claim: 3.3566775244299674\n",
      "Label distribution: Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
      "\n",
      "Total evidence paragraphs: 1208827\n",
      "Max evidence length: 3148\n",
      "Min evidence length: 1\n",
      "Mean evidence length: 119.51412319546138\n"
     ]
    }
   ],
   "source": [
    "def summarize_train(train_data):\n",
    "    claim_lengths = [len(c[\"claim_text\"]) for c in train_data.values()]\n",
    "    evidence_counts = [len(c[\"evidences\"]) for c in train_data.values()]\n",
    "    labels = [c[\"claim_label\"] for c in train_data.values()]\n",
    "\n",
    "    print(f\"\\nTrain claim count: {len(train_data)}\")\n",
    "    print(f\"Max claim length: {max(claim_lengths)}\")\n",
    "    print(f\"Min claim length: {min(claim_lengths)}\")\n",
    "    print(f\"Mean claim length: {np.mean(claim_lengths)}\")\n",
    "\n",
    "    print(f\"Max evidence count per claim: {max(evidence_counts)}\")\n",
    "    print(f\"Min evidence count per claim: {min(evidence_counts)}\")\n",
    "    print(f\"Mean evidence count per claim: {np.mean(evidence_counts)}\")\n",
    "\n",
    "    print(f\"Label distribution: {Counter(labels)}\")\n",
    "\n",
    "def summarize_evidence(evidence_data):\n",
    "    evidence_lengths = [len(evi) for evi in evidence_data.values()]\n",
    "    print(f\"\\nTotal evidence paragraphs: {len(evidence_data)}\")\n",
    "    print(f\"Max evidence length: {max(evidence_lengths)}\")\n",
    "    print(f\"Min evidence length: {min(evidence_lengths)}\")\n",
    "    print(f\"Mean evidence length: {np.mean(evidence_lengths)}\")\n",
    "\n",
    "summarize_train(train_data)\n",
    "summarize_evidence(evidence_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_and_tokenize_text(\n",
    "    text,\n",
    "    remove_stopwords=False,\n",
    "    lemmatize=False,\n",
    "    stem=False,\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    pos_aware_lemmatization=False,\n",
    "    replace_numbers=False,\n",
    "    normalize_unicode=False,\n",
    "    normalize_whitespace=False,\n",
    "    return_pos_tags=False\n",
    "):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    if normalize_unicode: # Normalization Form Compatibility Composition e.g. (1 / 2km -> 1/2km)\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "    \n",
    "    if normalize_whitespace: # remove tabs / whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if replace_numbers: # regard all numbers as same token <NUM>, when not care about different numbers meaning\n",
    "        tokens = [\"<NUM>\" if token.isdigit() else token for token in tokens]\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if pos_aware_lemmatization:\n",
    "            tagged = pos_tag(tokens)\n",
    "            tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged]\n",
    "        else:\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "    if return_pos_tags: # part of speech \n",
    "        tagged = pos_tag(tokens)\n",
    "        return list(zip(tokens, [tag for _, tag in tagged]))\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct the dataframe to store the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = [v[\"claim_text\"] for v in train_data.values()]\n",
    "Y_train_evidence = [v[\"evidences\"] for v in train_data.values()]\n",
    "Y_train_label = [v[\"claim_label\"] for v in train_data.values()]\n",
    "\n",
    "X_dev_text = [v[\"claim_text\"] for v in dev_data.values()]\n",
    "Y_dev_evidence = [v[\"evidences\"] for v in dev_data.values()]\n",
    "Y_dev_label = [v[\"claim_label\"] for v in dev_data.values()]\n",
    "\n",
    "X_test_text = [v[\"claim_text\"] for v in test_data.values()]\n",
    "\n",
    "evidence_content = [v for v in evidence_data.values()]\n",
    "\n",
    "# train dataset\n",
    "df_train = pd.DataFrame({\n",
    "    \"claim_id\": list(train_data.keys()), # claim id\n",
    "    \"claim_text\": X_train_text,\n",
    "    \"evidences\": Y_train_evidence,\n",
    "    \"label\": Y_train_label\n",
    "})\n",
    "\n",
    "# development dataframe\n",
    "df_dev = pd.DataFrame({\n",
    "    \"claim_id\": list(dev_data.keys()), # claim ID\n",
    "    \"claim_text\": X_dev_text,\n",
    "    \"evidences\": Y_dev_evidence,\n",
    "    \"label\": Y_dev_label\n",
    "})\n",
    "\n",
    "# test dataset\n",
    "df_test = pd.DataFrame({\n",
    "    \"claim_id\": list(test_data.keys()),\n",
    "    \"claim_text\": X_test_text\n",
    "})\n",
    "\n",
    "# evidence dataframe\n",
    "df_evidence = pd.DataFrame({\n",
    "    \"evidence_id\": list(evidence_data.keys()), # evidence ID\n",
    "    \"evidence_content\": evidence_content\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "# pairs is the list of tuples, tuple is (claim, evidence)\n",
    "# min_freq means that just remain the words appears two times\n",
    "def build_vocab(pairs, min_freq = 2):\n",
    "    # Count the frequency of words\n",
    "    counter = Counter()\n",
    "    # after text cleaning and split the text to word, count the frequency of words\n",
    "    for claim, evidence in pairs:\n",
    "        text = clean_and_tokenize_text(claim) + \" \" + clean_and_tokenize_text(evidence)\n",
    "        tokens = text.split()\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    # just remain the frequency of words is more than two times\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Claim and Evidence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum length for claim and evidence\n",
    "claim_lengths = [len(c[\"claim_text\"]) for c in train_data.values()]\n",
    "evidence_lengths = [len(evi) for evi in evidence_data.values()]\n",
    "max_len_claim_evi = np.mean(claim_lengths) + np.mean(evidence_lengths) + 1\n",
    "\n",
    "max_len_claim_evi = int(max_len_claim_evi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use self-defined pytorh dataset to change the text dataset\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    def __init__(self, pairs, labels, vocab, max_len = max_len_claim_evi):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    # get the frequency of pairs in dataset\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    \n",
    "    # convert the word to the number ids\n",
    "    def text_to_ids(self, text):\n",
    "        # convert the cleaning dataset to list of id\n",
    "        tokens = clean_and_tokenize_text(text).split()\n",
    "        ids = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens]\n",
    "        # if the length is not enough, then add '<PAD>'\n",
    "        # if the length is longer, the longer parts will be deleted\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [self.vocab[\"<PAD>\"]] * (self.max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:self.max_len]\n",
    "        return ids\n",
    "    \n",
    "    # get the claim and evidence of idx sample and add separate '[SEP]' between claim and evidence\n",
    "    def __getitem__(self, idx):\n",
    "        claim, evidence = self.pairs[idx]\n",
    "        input_ids = self.text_to_ids(claim + \" [SEP] \" + evidence)\n",
    "        return torch.tensor(input_ids), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            # ensures the padding token does not affect learning\n",
    "            padding_idx=padding_idx  \n",
    "        )\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "\n",
    "            # input shape will be (batch_size, seq_len, embedding_dim)\n",
    "            batch_first=True, \n",
    "                \n",
    "            bidirectional=False    \n",
    "        )\n",
    "\n",
    "        # Dropout layer: helps prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Fully connected layer: maps LSTM hidden states to output classes\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        # Convert token IDs to embeddings [B, T, E]\n",
    "        embedded = self.embedding(input_ids)\n",
    "\n",
    "        # Feed embeddings into the LSTM\n",
    "        # hidden: [1, B, H] (we ignore the full output sequence and the cell state)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "\n",
    "        # Remove the first dimension (num_layers = 1) to [B, H]\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(hidden)\n",
    "\n",
    "        # Final linear layer\n",
    "        output = self.fc(dropped)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_pairs(train_claims, evidence_dict):\n",
    "    #Mapping from textual labels to numeric labels for classification\n",
    "    LABEL_MAP = {\n",
    "        \"SUPPORTS\": 0,\n",
    "        \"REFUTES\": 1,\n",
    "        \"NOT_ENOUGH_INFO\": 2,\n",
    "        \"DISPUTED\": 3\n",
    "    }\n",
    "\n",
    "    #Initialize lists to store (claim, evidence) pairs and labels\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    #Iterate over all claims in the dataset\n",
    "    for cid, item in train_claims.items():\n",
    "        #Skip claims without a label or without evidence references\n",
    "        if \"claim_label\" not in item or \"evidences\" not in item:\n",
    "            continue\n",
    "\n",
    "        #Extract the claim text and convert its label to numeric\n",
    "        claim_text = item[\"claim_text\"]\n",
    "        label = LABEL_MAP[item[\"claim_label\"]]\n",
    "\n",
    "        #For each associated evidence ID, retrieve the evidence text and form a training pair\n",
    "        for eid in item[\"evidences\"]:\n",
    "            evidence_text = evidence_dict.get(eid, \"\")\n",
    "            pairs.append((claim_text, evidence_text))\n",
    "            labels.append(label)\n",
    "\n",
    "    #Return the list of claim-evidence pairs and their labels\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Pytorch model\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device):\n",
    "    # initialise the the model\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # start to train the epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        # loop the small batch of data from training dataset for training\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Then evaluate the model with validation dataset\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                preds = model(batch_x).argmax(dim=1)\n",
    "                total_correct += (preds == batch_y).sum().item()\n",
    "                total_count += len(batch_y)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}. Training Loss: {total_loss:.3f}. Validation Accuracy: {total_correct / total_count:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training Loss: 127.261. Validation Accuracy: 0.458\n",
      "Epoch 2. Training Loss: 124.532. Validation Accuracy: 0.458\n",
      "Epoch 3. Training Loss: 124.683. Validation Accuracy: 0.458\n",
      "Epoch 4. Training Loss: 125.923. Validation Accuracy: 0.458\n",
      "Epoch 5. Training Loss: 124.598. Validation Accuracy: 0.458\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #create training pairs\n",
    "    pairs, labels = build_training_pairs(train_data, evidence_data)\n",
    "\n",
    "    #create vocabulary\n",
    "    vocab = build_vocab(pairs)\n",
    "\n",
    "    #create dataset + dataLoader\n",
    "    full_dataset = ClaimEvidenceDataset(pairs, labels, vocab)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32)\n",
    "\n",
    "    #initialize model\n",
    "    model = LSTMClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=100,\n",
    "        hidden_dim=128,\n",
    "        output_dim=4,\n",
    "        padding_idx=vocab[\"<PAD>\"]\n",
    "    )\n",
    "\n",
    "    #train model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(model, train_loader, val_loader, num_epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TF-IDF and Cosine Similarity\n",
    "def retrieve_top_k_evidence_ids(claim_text, evidence_dict, k=5):\n",
    " \n",
    "    #Extract evidence IDs and their corresponding texts\n",
    "    ev_ids = list(evidence_dict.keys())\n",
    "    ev_texts = [evidence_dict[eid] for eid in ev_ids]\n",
    "\n",
    "    #Vectorize claim and all evidences using TF-IDF\n",
    "    all_texts = [claim_text] + ev_texts\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    tfidf = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "    # Separate claim vector and evidence vectors\n",
    "    claim_vec = tfidf[0]           # Vector for the claim\n",
    "    ev_vecs = tfidf[1:]            # Vectors for all evidences\n",
    "\n",
    "    #Compute cosine similarity between claim and each evidence\n",
    "    sims = cosine_similarity(claim_vec, ev_vecs).flatten()\n",
    "\n",
    "    #Get indices of top-k highest similarity scores\n",
    "    top_k_idx = sims.argsort()[-k:][::-1]\n",
    "\n",
    "    #Return corresponding evidence IDs\n",
    "    return [ev_ids[i] for i in top_k_idx]\n",
    "\n",
    "\n",
    "def predict_and_write_json(model, data_dict, evidence_dict, vocab, output_path, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Mapping from numeric label to text label\n",
    "    ID2LABEL = {0: \"SUPPORTS\", 1: \"REFUTES\", 2: \"NOT_ENOUGH_INFO\", 3: \"DISPUTED\"}\n",
    "    results = {}\n",
    "\n",
    "    for claim_id, item in data_dict.items():\n",
    "        claim_text = item[\"claim_text\"]\n",
    "\n",
    "        #Retrieve top-k similar evidences using TF-IDF\n",
    "        evidence_ids = retrieve_top_k_evidence_ids(claim_text, evidence_dict, k=5)\n",
    "\n",
    "        #Connect the first top-3 evidence\n",
    "        evidence_texts = [evidence_dict.get(eid, \"\") for eid in evidence_ids[:3]]\n",
    "        evidence_text = \" \".join(evidence_texts)\n",
    "\n",
    "\n",
    "        #Convert text to token IDs\n",
    "        tokens = clean_and_tokenize_text(claim_text + \" [SEP] \" + evidence_text).split()\n",
    "        ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
    "        ids = ids[:100] + [vocab[\"<PAD>\"]] * max(0, 100 - len(ids))\n",
    "\n",
    "        input_tensor = torch.tensor([ids]).to(device)\n",
    "\n",
    "        #Run the model to get prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "            pred_id = logits.argmax(dim=1).item()\n",
    "\n",
    "        #Store prediction result\n",
    "        results[claim_id] = {\n",
    "            \"claim_label\": ID2LABEL[pred_id],\n",
    "            \"evidences\": evidence_ids[:5]\n",
    "        }\n",
    "\n",
    "    #Write all predictions to a JSON file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training Loss: 125.293. Validation Accuracy: 0.472\n",
      "Epoch 2. Training Loss: 123.688. Validation Accuracy: 0.472\n",
      "Epoch 3. Training Loss: 125.107. Validation Accuracy: 0.472\n",
      "Epoch 4. Training Loss: 123.902. Validation Accuracy: 0.472\n",
      "Epoch 5. Training Loss: 124.020. Validation Accuracy: 0.472\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # get the training pairs and labels\n",
    "    pairs, labels = build_training_pairs(train_data, evidence_data)\n",
    "    # create the vocabulary from training pairs\n",
    "    vocab = build_vocab(pairs)\n",
    "    \n",
    "    # split the 80% training datasets and 20% validation datasets\n",
    "    full_dataset = ClaimEvidenceDataset(pairs, labels, vocab)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "    # use dataloader to batch training datasets and validation datasets\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32)\n",
    "\n",
    "    # initialize the LSTM model\n",
    "    model = LSTMClassifier(\n",
    "        vocab_size = len(vocab),\n",
    "        embedding_dim = 100,\n",
    "        hidden_dim = 128,\n",
    "        output_dim = 4,\n",
    "        padding_idx = vocab[\"<PAD>\"]\n",
    "    )\n",
    "\n",
    "    # select the device to train the model\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "    else:\n",
    "        device_type = \"cpu\"\n",
    "    device = torch.device(device_type)\n",
    "    train_model(model, train_loader, val_loader, num_epochs=5, device=device)\n",
    "\n",
    "    # predict the dev set and write in json file\n",
    "    predict_and_write_json(\n",
    "        model = model,\n",
    "        data_dict = dev_data,    \n",
    "        evidence_dict = evidence_data,\n",
    "        vocab = vocab,\n",
    "        output_path = \"dev_pred.json\",\n",
    "        device = device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            # ensures the padding token does not affect learning\n",
    "            padding_idx=padding_idx  \n",
    "        )\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "\n",
    "            # input shape will be (batch_size, seq_len, embedding_dim)\n",
    "            batch_first=True, \n",
    "            # change the bidirectional is True\n",
    "            bidirectional=True   \n",
    "        )\n",
    "\n",
    "        # Dropout layer: helps prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Fully connected layer: maps LSTM hidden states to output classes\n",
    "        # Change the hidden states become 2, beacuse here is bidirectional\n",
    "        self.fc = nn.Linear(in_features=hidden_dim * 2, out_features=output_dim)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        # Convert token IDs to embeddings [B, T, E]\n",
    "        embedded = self.embedding(input_ids)\n",
    "\n",
    "        # Feed embeddings into the LSTM\n",
    "        # hidden: [1, B, H] (we ignore the full output sequence and the cell state)\n",
    "        _, (hidden, _) = self.lstm(embedded) \n",
    "        \n",
    "        # hidden is bidirectional is [B, 2*H]\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(hidden)\n",
    "\n",
    "        # Final linear layer\n",
    "        output = self.fc(dropped) \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_pairs(train_claims, evidence_dict):\n",
    "    #Mapping from textual labels to numeric labels for classification\n",
    "    LABEL_MAP = {\n",
    "        \"SUPPORTS\": 0,\n",
    "        \"REFUTES\": 1,\n",
    "        \"NOT_ENOUGH_INFO\": 2,\n",
    "        \"DISPUTED\": 3\n",
    "    }\n",
    "\n",
    "    #Initialize lists to store (claim, evidence) pairs and labels\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    #Iterate over all claims in the dataset\n",
    "    for cid, item in train_claims.items():\n",
    "        #Skip claims without a label or without evidence references\n",
    "        if \"claim_label\" not in item or \"evidences\" not in item:\n",
    "            continue\n",
    "\n",
    "        #Extract the claim text and convert its label to numeric\n",
    "        claim_text = item[\"claim_text\"]\n",
    "        label = LABEL_MAP[item[\"claim_label\"]]\n",
    "\n",
    "        #For each associated evidence ID, retrieve the evidence text and form a training pair\n",
    "        for eid in item[\"evidences\"]:\n",
    "            evidence_text = evidence_dict.get(eid, \"\")\n",
    "            pairs.append((claim_text, evidence_text))\n",
    "            labels.append(label)\n",
    "\n",
    "    #Return the list of claim-evidence pairs and their labels\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Pytorch model\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device):\n",
    "    # initialise the the model\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # start to train the epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        # loop the small batch of data from training dataset for training\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Then evaluate the model with validation dataset\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                preds = model(batch_x).argmax(dim=1)\n",
    "                total_correct += (preds == batch_y).sum().item()\n",
    "                total_count += len(batch_y)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}. Training Loss: {total_loss:.3f}. Validation Accuracy: {total_correct / total_count:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training Loss: 119.555. Validation Accuracy: 0.587\n",
      "Epoch 2. Training Loss: 94.493. Validation Accuracy: 0.732\n",
      "Epoch 3. Training Loss: 59.411. Validation Accuracy: 0.796\n",
      "Epoch 4. Training Loss: 37.382. Validation Accuracy: 0.887\n",
      "Epoch 5. Training Loss: 20.888. Validation Accuracy: 0.920\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #create training pairs\n",
    "    pairs, labels = build_training_pairs(train_data, evidence_data)\n",
    "\n",
    "    #create vocabulary\n",
    "    vocab = build_vocab(pairs)\n",
    "\n",
    "    #create dataset + dataLoader\n",
    "    full_dataset = ClaimEvidenceDataset(pairs, labels, vocab)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32)\n",
    "\n",
    "    #initialize model\n",
    "    model = LSTMClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=100,\n",
    "        hidden_dim=128,\n",
    "        output_dim=4,\n",
    "        padding_idx=vocab[\"<PAD>\"]\n",
    "    )\n",
    "\n",
    "    #train model\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "    else:\n",
    "        device_type = \"cpu\"\n",
    "    device = torch.device(device_type)\n",
    "    train_model(model, train_loader, val_loader, num_epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TF-IDF and Cosine Similarity\n",
    "def retrieve_top_k_evidence_ids(claim_text, evidence_dict, k=5):\n",
    " \n",
    "    #Extract evidence IDs and their corresponding texts\n",
    "    ev_ids = list(evidence_dict.keys())\n",
    "    ev_texts = [evidence_dict[eid] for eid in ev_ids]\n",
    "\n",
    "    #Vectorize claim and all evidences using TF-IDF\n",
    "    all_texts = [claim_text] + ev_texts\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    tfidf = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "    #Separate claim vector and evidence vectors\n",
    "    claim_vec = tfidf[0]\n",
    "    ev_vecs = tfidf[1:]  \n",
    "\n",
    "    #Compute cosine similarity between claim and each evidence\n",
    "    sims = cosine_similarity(claim_vec, ev_vecs).flatten()\n",
    "\n",
    "    #Get indices of top-k highest similarity scores\n",
    "    top_k_idx = sims.argsort()[-k:][::-1]\n",
    "\n",
    "    return [ev_ids[i] for i in top_k_idx]\n",
    "\n",
    "\n",
    "def predict_and_write_json(model, data_dict, evidence_dict, vocab, output_path, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Mapping from numeric label to text label\n",
    "    ID2LABEL = {0: \"SUPPORTS\", 1: \"REFUTES\", 2: \"NOT_ENOUGH_INFO\", 3: \"DISPUTED\"}\n",
    "    results = {}\n",
    "\n",
    "    for claim_id, item in data_dict.items():\n",
    "        claim_text = item[\"claim_text\"]\n",
    "\n",
    "        #Retrieve top-k similar evidences using TF-IDF\n",
    "        evidence_ids = retrieve_top_k_evidence_ids(claim_text, evidence_dict, k=5)\n",
    "\n",
    "        #Connect the first top-3 evidence\n",
    "        evidence_text = evidence_dict.get(evidence_ids[0], \"\")\n",
    "\n",
    "        #Convert text to token IDs\n",
    "        tokens = clean_and_tokenize_text(claim_text + \" [SEP] \" + evidence_text).split()\n",
    "        ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
    "        ids = ids[:100] + [vocab[\"<PAD>\"]] * max(0, 100 - len(ids))\n",
    "\n",
    "        input_tensor = torch.tensor([ids]).to(device)\n",
    "\n",
    "        #Run the model to get prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "            pred_id = logits.argmax(dim=1).item()\n",
    "\n",
    "        #Store prediction result\n",
    "        results[claim_id] = {\n",
    "            \"claim_label\": ID2LABEL[pred_id],\n",
    "            \"evidences\": evidence_ids[:5]\n",
    "        }\n",
    "\n",
    "    #Write all predictions to a JSON file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training Loss: 119.587. Validation Accuracy: 0.572\n",
      "Epoch 2. Training Loss: 91.762. Validation Accuracy: 0.698\n",
      "Epoch 3. Training Loss: 54.661. Validation Accuracy: 0.813\n",
      "Epoch 4. Training Loss: 30.520. Validation Accuracy: 0.891\n",
      "Epoch 5. Training Loss: 16.540. Validation Accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # get the training pairs and labels\n",
    "    pairs, labels = build_training_pairs(train_data, evidence_data)\n",
    "    # create the vocabulary from training pairs\n",
    "    vocab = build_vocab(pairs)\n",
    "    \n",
    "    # split the 80% training datasets and 20% validation datasets\n",
    "    full_dataset = ClaimEvidenceDataset(pairs, labels, vocab)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "    # use dataloader to batch training datasets and validation datasets\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32)\n",
    "\n",
    "    # initialize the LSTM model\n",
    "    model = LSTMClassifier(\n",
    "        vocab_size = len(vocab),\n",
    "        embedding_dim = 100,\n",
    "        hidden_dim = 128,\n",
    "        output_dim = 4,\n",
    "        padding_idx = vocab[\"<PAD>\"]\n",
    "    )\n",
    "    \n",
    "    # select the device to train the model\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "    else:\n",
    "        device_type = \"cpu\"\n",
    "    device = torch.device(device_type)\n",
    "    train_model(model, train_loader, val_loader, num_epochs=5, device=device)\n",
    "\n",
    "    # predict the dev set and write in json file\n",
    "    predict_and_write_json(\n",
    "        model = model,\n",
    "        data_dict = dev_data,    \n",
    "        evidence_dict = evidence_data,\n",
    "        vocab = vocab,\n",
    "        output_path = \"dev_pred_bidirect.json\",\n",
    "        device = device\n",
    "    )  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/duhaozhou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/duhaozhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/duhaozhou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/duhaozhou/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Exploratory Data Analysis (EDA session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a). load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_all_data(data_dir='data'):\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    train_claims = load_json_data(data_dir / 'train-claims.json')\n",
    "    dev_claims = load_json_data(data_dir / 'dev-claims.json')\n",
    "    test_claims = load_json_data(data_dir / 'test-claims-unlabelled.json')\n",
    "    evidences = load_json_data(data_dir / 'evidence.json')\n",
    "\n",
    "    return train_claims, dev_claims, test_claims, evidences\n",
    "train_data, dev_data, test_data, evidence_data = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b). Breif summary of train dataset & evidence dataset (Max /Min /Mean / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train claim count: 1228\n",
      "Max claim length: 332\n",
      "Min claim length: 26\n",
      "Mean claim length: 122.95521172638436\n",
      "Max evidence count per claim: 5\n",
      "Min evidence count per claim: 1\n",
      "Mean evidence count per claim: 3.3566775244299674\n",
      "Label distribution: Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
      "\n",
      "Total evidence paragraphs: 1208827\n",
      "Max evidence length: 3148\n",
      "Min evidence length: 1\n",
      "Mean evidence length: 119.51412319546138\n"
     ]
    }
   ],
   "source": [
    "def summarize_train(train_data):\n",
    "    claim_lengths = [len(c[\"claim_text\"]) for c in train_data.values()]\n",
    "    evidence_counts = [len(c[\"evidences\"]) for c in train_data.values()]\n",
    "    labels = [c[\"claim_label\"] for c in train_data.values()]\n",
    "\n",
    "    print(f\"\\nTrain claim count: {len(train_data)}\")\n",
    "    print(f\"Max claim length: {max(claim_lengths)}\")\n",
    "    print(f\"Min claim length: {min(claim_lengths)}\")\n",
    "    print(f\"Mean claim length: {np.mean(claim_lengths)}\")\n",
    "\n",
    "    print(f\"Max evidence count per claim: {max(evidence_counts)}\")\n",
    "    print(f\"Min evidence count per claim: {min(evidence_counts)}\")\n",
    "    print(f\"Mean evidence count per claim: {np.mean(evidence_counts)}\")\n",
    "\n",
    "    print(f\"Label distribution: {Counter(labels)}\")\n",
    "\n",
    "def summarize_evidence(evidence_data):\n",
    "    evidence_lengths = [len(evi) for evi in evidence_data.values()]\n",
    "    print(f\"\\nTotal evidence paragraphs: {len(evidence_data)}\")\n",
    "    print(f\"Max evidence length: {max(evidence_lengths)}\")\n",
    "    print(f\"Min evidence length: {min(evidence_lengths)}\")\n",
    "    print(f\"Mean evidence length: {np.mean(evidence_lengths)}\")\n",
    "\n",
    "summarize_train(train_data)\n",
    "summarize_evidence(evidence_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Hugging Face BERT Embedding & Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Huggingface Transformers for model loading and inference in the context of in-context learning\n",
    "if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "else:\n",
    "    device_type = \"cpu\"\n",
    "device = torch.device(device_type)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "bert_model.eval()\n",
    "\n",
    "#Transformer Classifier\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=512, num_classes=4, num_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=4, \n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, cls_embed): \n",
    "        #[B, 1, input_dim]\n",
    "        x = self.encoder(cls_embed)\n",
    "        #Use [CLS] token only\n",
    "        return self.classifier(x[:, 0])\n",
    "\n",
    "#Dataset\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, pairs, labels):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claim, evidence = self.pairs[idx]\n",
    "        text = claim + \" [SEP] \" + evidence\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "#Bert embedding\n",
    "def get_cls_embedding(texts, tokenizer, model, device):\n",
    "    #Set the maximum length is 128, because claims and evidences are short text\n",
    "    encoding = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "        return output.last_hidden_state[:, 0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, bert, train_loader, val_loader, num_epochs, device):\n",
    "    # put model on GPU or CPU\n",
    "    model.to(device)\n",
    "    # the learning rate is from the default value 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # start Training the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            texts = batch[\"text\"]\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            # get the Bert's Embeddings as input factors\n",
    "            cls_embed = get_cls_embedding(texts, tokenizer, bert, device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(cls_embed)\n",
    "            # compute classification loss\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "            # add the loss for each loop\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # this is the validation part\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_count = 0\n",
    "        # the calculation of disable gradient\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                texts = batch[\"text\"]\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                cls_embed = get_cls_embedding(texts, tokenizer, bert, device)\n",
    "                preds = model(cls_embed).argmax(dim=1)\n",
    "                total_correct += (preds == labels).sum().item()\n",
    "                total_count += len(labels)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {total_loss:.3f}, Validation Accuracy = {total_correct / total_count:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TF-IDF and Cosine Similarity\n",
    "def retrieve_top_k_evidence_ids(claim_text, evidence_dict, k=5):\n",
    "    \n",
    "    #Extract evidence IDs and their corresponding texts\n",
    "    ev_ids = list(evidence_dict.keys())\n",
    "    ev_texts = [evidence_dict[eid] for eid in ev_ids]\n",
    "    \n",
    "    #Vectorize claim and all evidences using TF-IDF\n",
    "    all_texts = [claim_text] + ev_texts\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    tfidf = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "    # Separate claim vector and evidence vectors\n",
    "    claim_vec = tfidf[0]\n",
    "    ev_vecs = tfidf[1:]\n",
    "    \n",
    "    #Compute cosine similarity between claim and each evidence\n",
    "    sims = cosine_similarity(claim_vec, ev_vecs).flatten()\n",
    "    \n",
    "    #Get indices of top-k highest similarity scores\n",
    "    top_k_idx = sims.argsort()[-k:][::-1]\n",
    "    \n",
    "    return [ev_ids[i] for i in top_k_idx]\n",
    "\n",
    "\n",
    "def predict_and_write_json(model, data_dict, evidence_dict, output_path, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    #Mapping from numeric label to text label\n",
    "    ID2LABEL = {0: \"SUPPORTS\", 1: \"REFUTES\", 2: \"NOT_ENOUGH_INFO\", 3: \"DISPUTED\"}\n",
    "    results = {}\n",
    "\n",
    "    for claim_id, item in data_dict.items():\n",
    "        claim_text = item[\"claim_text\"]\n",
    "\n",
    "        #Retrieve top-k similar evidences using TF-IDF\n",
    "        evidence_ids = retrieve_top_k_evidence_ids(claim_text, evidence_dict, k=5)\n",
    "        \n",
    "        #Connect the first top-3 evidence\n",
    "        evidence_texts = [evidence_dict.get(eid, \"\") for eid in evidence_ids[:3]]\n",
    "        combined_text = claim_text + \" [SEP] \" + \" \".join(evidence_texts)\n",
    "\n",
    "        #Get the Bert's cls embedding\n",
    "        cls_embed = get_cls_embedding([combined_text], tokenizer, bert_model, device)\n",
    "        \n",
    "        #Run the model to get prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(cls_embed)\n",
    "            pred_id = logits.argmax(dim=1).item()\n",
    "    \n",
    "        #Store prediction result\n",
    "        results[claim_id] = {\n",
    "            \"claim_label\": ID2LABEL[pred_id],\n",
    "            \"evidences\": evidence_ids[:3]\n",
    "        }\n",
    "    \n",
    "    #Write all predictions to a JSON file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 273.363, Validation Accuracy = 0.492\n",
      "Epoch 2: Training Loss = 253.531, Validation Accuracy = 0.492\n",
      "Epoch 3: Training Loss = 254.710, Validation Accuracy = 0.492\n",
      "Epoch 4: Training Loss = 251.255, Validation Accuracy = 0.492\n",
      "Epoch 5: Training Loss = 249.477, Validation Accuracy = 0.492\n"
     ]
    }
   ],
   "source": [
    "# get the claim and evidence pairs from training original datasets\n",
    "# return the list of pairs and labels for training\n",
    "def build_training_pairs(claims_dict, evidence_dict):\n",
    "    pairs, labels = [], []\n",
    "    label_map = {\"SUPPORTS\": 0, \n",
    "                 \"REFUTES\": 1, \n",
    "                 \"NOT_ENOUGH_INFO\": 2, \n",
    "                 \"DISPUTED\": 3}\n",
    "    \n",
    "    # iterate of each claim\n",
    "    for item in claims_dict.values():\n",
    "        claim = item[\"claim_text\"]\n",
    "        lbl = label_map[item[\"claim_label\"]]\n",
    "        \n",
    "        # when claims with linked evidence IDs\n",
    "        if item[\"evidences\"]:\n",
    "            for eid in item[\"evidences\"]:\n",
    "                ev_text = evidence_dict.get(eid, \"\")\n",
    "                \n",
    "                # for non-empty evidence, then add their pairs and labels\n",
    "                if ev_text:\n",
    "                    pairs.append((claim, ev_text))\n",
    "                    labels.append(lbl)\n",
    "                    \n",
    "    return pairs, labels\n",
    "\n",
    "# create the training pairs and labels for Pytorch datasets\n",
    "pairs, labels = build_training_pairs(train_data, evidence_data)\n",
    "dataset = BERTDataset(pairs, labels)\n",
    "\n",
    "# split the dataset with 0.8 training and 0.2 validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_set, val_set = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16)\n",
    "\n",
    "# train the transformer model and predit labels for dev set to json file\n",
    "model = Transformer()\n",
    "train_model(model, bert_model, train_loader, val_loader, num_epochs=5, device=device)\n",
    "predict_and_write_json(model, dev_data, evidence_data, \"dev_pred_bert_large.json\", device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

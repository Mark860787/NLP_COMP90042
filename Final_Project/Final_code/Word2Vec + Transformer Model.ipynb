{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tianhao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tianhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tianhao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tianhao/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Import necessary libraries and setup environment\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running in local environment\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1228 training claims, 154 development claims, 153 test claims, and 1208827 evidence passages\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Load data functions\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"Load JSON data from file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_all_data(data_dir='data'):\n",
    "    \"\"\"Load all necessary data files\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    train_claims = load_json_data(data_dir / 'train-claims.json')\n",
    "    dev_claims = load_json_data(data_dir / 'dev-claims.json')\n",
    "    test_claims = load_json_data(data_dir / 'test-claims-unlabelled.json')\n",
    "    evidences = load_json_data(data_dir / 'evidence.json')\n",
    "    return train_claims, dev_claims, test_claims, evidences\n",
    "    \n",
    "try:\n",
    "    train_data, dev_data, test_data, evidence_data = load_all_data()\n",
    "    print(f\"Loaded {len(train_data)} training claims, {len(dev_data)} development claims, {len(test_data)} test claims, and {len(evidence_data)} evidence passages\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure data files are in the 'data' directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Text preprocessing functions\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert Penn Treebank POS tags to WordNet POS tags\"\"\"\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_and_tokenize_text(\n",
    "    text,\n",
    "    remove_stopwords=False,\n",
    "    lemmatize=False,\n",
    "    stem=False,\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    pos_aware_lemmatization=False,\n",
    "    replace_numbers=False,\n",
    "    normalize_unicode=False,\n",
    "    normalize_whitespace=False,\n",
    "    return_pos_tags=False\n",
    "):\n",
    "    \"\"\"Clean and tokenize text with various preprocessing options\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    if normalize_unicode:\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "    \n",
    "    if normalize_whitespace:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if replace_numbers:\n",
    "        tokens = [\"<NUM>\" if token.isdigit() else token for token in tokens]\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if pos_aware_lemmatization:\n",
    "            tagged = pos_tag(tokens)\n",
    "            tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged]\n",
    "        else:\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "    if return_pos_tags:\n",
    "        tagged = pos_tag(tokens)\n",
    "        return list(zip(tokens, [tag for _, tag in tagged]))\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Google News Word2Vec model...\n",
      "Word2Vec model already loaded\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Load Word2Vec model\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"Loading Google News Word2Vec model...\")\n",
    "try:\n",
    "    # First, check if we already have the model in memory\n",
    "    try:\n",
    "        word2vec\n",
    "        print(\"Word2Vec model already loaded\")\n",
    "    except NameError:\n",
    "        # Try to load the model\n",
    "        word2vec = api.load('word2vec-google-news-300')\n",
    "        print(\"Word2Vec model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Word2Vec model: {e}\")\n",
    "    \n",
    "    # Fallback to simple word embeddings if Word2Vec fails\n",
    "    print(\"Falling back to simple TF-IDF representation\")\n",
    "    use_tfidf_fallback = True\n",
    "else:\n",
    "    use_tfidf_fallback = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Word2Vec embedding extraction and SVD dimensionality reduction\n",
    "def get_word2vec_embedding(text, word2vec_model, dim=300):\n",
    "    \"\"\"Get average Word2Vec vector representation for text\"\"\"\n",
    "    tokens = clean_and_tokenize_text(text).split()\n",
    "    vectors = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in word2vec_model:\n",
    "            vectors.append(word2vec_model[token])\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)\n",
    "    \n",
    "    # Average embeddings\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "def create_embeddings_with_svd(texts, word2vec_model, n_components=256):\n",
    "    \"\"\"Create embeddings for all texts and apply SVD dimensionality reduction\"\"\"\n",
    "    # Get raw embeddings\n",
    "    print(f\"Creating embeddings for {len(texts)} texts...\")\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        embedding = get_word2vec_embedding(text, word2vec_model)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Apply SVD\n",
    "    print(f\"Applying SVD dimensionality reduction to {embeddings.shape}...\")\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    reduced_embeddings = svd.fit_transform(embeddings)\n",
    "    \n",
    "    print(f\"SVD complete. Shape after reduction: {reduced_embeddings.shape}\")\n",
    "    return reduced_embeddings, svd\n",
    "\n",
    "# TF-IDF fallback if Word2Vec fails\n",
    "def create_tfidf_embeddings(texts, n_components=256):\n",
    "    \"\"\"Create TF-IDF embeddings with SVD dimensionality reduction\"\"\"\n",
    "    print(f\"Creating TF-IDF embeddings for {len(texts)} texts...\")\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        preprocessor=lambda x: clean_and_tokenize_text(\n",
    "            x, remove_stopwords=True, lemmatize=True\n",
    "        ),\n",
    "        max_features=10000\n",
    "    )\n",
    "    \n",
    "    # Fit and transform texts\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Apply SVD\n",
    "    print(f\"Applying SVD dimensionality reduction to {tfidf_matrix.shape}...\")\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    reduced_embeddings = svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    print(f\"SVD complete. Shape after reduction: {reduced_embeddings.shape}\")\n",
    "    return reduced_embeddings, svd, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Evidence retrieval functions\n",
    "def retrieve_top_k_evidence_word2vec(claim_text, evidence_dict, word2vec_model, svd_model, evidence_embeddings, k=5):\n",
    "    \"\"\"Retrieve top-k evidence using Word2Vec embeddings and cosine similarity\"\"\"\n",
    "    # Get claim embedding\n",
    "    claim_embedding = get_word2vec_embedding(claim_text, word2vec_model)\n",
    "    claim_embedding = claim_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Apply SVD reduction\n",
    "    claim_embedding_reduced = svd_model.transform(claim_embedding)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(claim_embedding_reduced, evidence_embeddings).flatten()\n",
    "    \n",
    "    # Get top-k evidence IDs\n",
    "    ev_ids = list(evidence_dict.keys())\n",
    "    top_k_idx = similarities.argsort()[-k:][::-1]\n",
    "    \n",
    "    # Return evidence IDs and their similarities for re-ranking\n",
    "    return [(ev_ids[i], similarities[i]) for i in top_k_idx]\n",
    "\n",
    "def retrieve_top_k_evidence_tfidf(claim_text, evidence_dict, vectorizer, svd_model, evidence_embeddings, k=5):\n",
    "    \"\"\"Retrieve top-k evidence using TF-IDF embeddings and cosine similarity\"\"\"\n",
    "    # Get claim embedding\n",
    "    claim_vector = vectorizer.transform([claim_text])\n",
    "    claim_embedding_reduced = svd_model.transform(claim_vector)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(claim_embedding_reduced, evidence_embeddings).flatten()\n",
    "    \n",
    "    # Get top-k evidence IDs\n",
    "    ev_ids = list(evidence_dict.keys())\n",
    "    top_k_idx = similarities.argsort()[-k:][::-1]\n",
    "    \n",
    "    # Return evidence IDs and their similarities for re-ranking\n",
    "    return [(ev_ids[i], similarities[i]) for i in top_k_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerReranker(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #input projection level\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        #Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # use attention pooling to substitute average pooling\n",
    "        self.attention_pooling = AttentionPooling(hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # reranking\n",
    "        self.ranking_head = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.classification_head = nn.Linear(hidden_dim, 4)\n",
    "        \n",
    "    def forward(self, x, task='ranking', return_attention=False):\n",
    "       \n",
    "        x = self.input_projection(x)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        pooled_x, attention_weights = self.attention_pooling(x)\n",
    "        pooled_x = self.dropout(pooled_x)\n",
    "        \n",
    "        if task == 'ranking':\n",
    "            \n",
    "            logits = self.ranking_head(pooled_x)\n",
    "            # use log-sigmoid to improve stability\n",
    "            result = F.logsigmoid(logits)\n",
    "        else:\n",
    "            \n",
    "            result = F.log_softmax(self.classification_head(pooled_x), dim=1)\n",
    "        \n",
    "        if return_attention:\n",
    "            return result, attention_weights\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_dim, attention_dim=None):\n",
    "        super().__init__()\n",
    "        if attention_dim is None:\n",
    "            attention_dim = hidden_dim // 2\n",
    "        \n",
    "        # attention calculation\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, attention_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "       \n",
    "        # calculate attention weight\n",
    "        attn_weights = self.attention(x)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        \n",
    "        weighted_output = x * attn_weights\n",
    "        \n",
    "        \n",
    "        output = weighted_output.sum(dim=1)\n",
    "        return output, attn_weights.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Dataset for re-ranking\n",
    "class RerankingDataset(Dataset):\n",
    "    def __init__(self, claim_evidence_pairs, labels, embedding_func, embedding_params):\n",
    "      \n",
    "        self.pairs = claim_evidence_pairs\n",
    "        self.labels = labels\n",
    "        self.embedding_func = embedding_func\n",
    "        self.embedding_params = embedding_params\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        claim, evidence = self.pairs[idx]\n",
    "        \n",
    "        # Get embeddings using the provided function\n",
    "        embedding = self.embedding_func(claim, evidence, **self.embedding_params)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return embedding_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 9: Embedding functions for the dataset\n",
    "def get_word2vec_combined_embedding(claim, evidence, word2vec_model, svd_model):\n",
    "    \n",
    "    claim_embedding = get_word2vec_embedding(claim, word2vec_model)\n",
    "    evidence_embedding = get_word2vec_embedding(evidence, word2vec_model)\n",
    "    \n",
    "    # Concatenate and reshape\n",
    "    combined_embedding = np.concatenate([claim_embedding, evidence_embedding])\n",
    "    combined_embedding = combined_embedding.reshape(2, -1)\n",
    "    \n",
    "    # Apply SVD reduction\n",
    "    combined_embedding_reduced = svd_model.transform(combined_embedding)\n",
    "    \n",
    "    return combined_embedding_reduced\n",
    "\n",
    "def get_tfidf_combined_embedding(claim, evidence, vectorizer, svd_model):\n",
    "    claim_vector = vectorizer.transform([claim])\n",
    "    evidence_vector = vectorizer.transform([evidence])\n",
    "    \n",
    "    claim_embedding = svd_model.transform(claim_vector)[0]\n",
    "    evidence_embedding = svd_model.transform(evidence_vector)[0]\n",
    "    \n",
    "    # Stack embeddings\n",
    "    combined_embedding = np.vstack([claim_embedding, evidence_embedding])\n",
    "    \n",
    "    return combined_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 10: Training function for re-ranker\n",
    "def train_reranker(model, train_loader, val_loader, num_epochs, device, patience=4):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    # Use BCE with logits loss for numerical stability\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(batch_x, task='ranking').squeeze()\n",
    "            # Convert log probabilities to actual probabilities for the loss\n",
    "            probs = log_probs.exp()\n",
    "            loss = criterion(log_probs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Training metrics\n",
    "            preds = (probs > 0.5).float()\n",
    "            total_correct += (preds == batch_y).sum().item()\n",
    "            total_count += len(batch_y)\n",
    "            total_loss += loss.item() * len(batch_y)\n",
    "        \n",
    "        train_loss = total_loss / total_count\n",
    "        train_acc = total_correct / total_count\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_total_correct = 0\n",
    "        val_total_count = 0\n",
    "        val_total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in tqdm(val_loader, desc=\"Validation\"):\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                log_probs = model(batch_x, task='ranking').squeeze()\n",
    "                # Convert log probabilities to actual probabilities\n",
    "                probs = log_probs.exp()\n",
    "                loss = criterion(log_probs, batch_y)\n",
    "                preds = (probs > 0.5).float()\n",
    "                val_total_correct += (preds == batch_y).sum().item()\n",
    "                val_total_count += len(batch_y)\n",
    "                val_total_loss += loss.item() * len(batch_y)\n",
    "        \n",
    "        val_loss = val_total_loss / val_total_count\n",
    "        val_acc = val_total_correct / val_total_count\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_reranker.pth')\n",
    "            print(\"  Saved new best model\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement for {patience_counter} epochs\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_reranker.pth'))\n",
    "    return model, best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 11: Build training data for re-ranking\n",
    "def build_reranking_data(train_claims, evidence_dict):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    for cid, item in tqdm(train_claims.items(), desc=\"Building training data\"):\n",
    "        if \"claim_label\" not in item or \"evidences\" not in item:\n",
    "            continue\n",
    "            \n",
    "        claim_text = item[\"claim_text\"]\n",
    "        relevant_evidence_ids = set(item[\"evidences\"])\n",
    "        \n",
    "        # Sample positive examples (relevant evidence)\n",
    "        for eid in relevant_evidence_ids:\n",
    "            if eid in evidence_dict:\n",
    "                evidence_text = evidence_dict[eid]\n",
    "                pairs.append((claim_text, evidence_text))\n",
    "                labels.append(1.0)  # Relevant\n",
    "        \n",
    "        # Sample negative examples (random evidence)\n",
    "        all_evidence_ids = list(evidence_dict.keys())\n",
    "        num_negatives = min(len(relevant_evidence_ids) * 3, 15)  # Increased negative samples\n",
    "        \n",
    "        for _ in range(num_negatives):\n",
    "            random_eid = random.choice(all_evidence_ids)\n",
    "            if random_eid not in relevant_evidence_ids:\n",
    "                evidence_text = evidence_dict[random_eid]\n",
    "                pairs.append((claim_text, evidence_text))\n",
    "                labels.append(0.0)  # Not relevant\n",
    "    \n",
    "    return pairs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=4, dropout=0.3):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # input projection\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # use attention pooling to substitute average pooling\n",
    "        self.attention_pooling = AttentionPooling(hidden_dim)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        \n",
    "    def forward(self, x, return_attention=False):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        \n",
    "        pooled_x, attention_weights = self.attention_pooling(x)\n",
    "        pooled_x = self.dropout(pooled_x)\n",
    "        \n",
    "        x = F.relu(self.fc1(pooled_x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        # use log-softmax to improve stability\n",
    "        result = F.log_softmax(logits, dim=1)\n",
    "        \n",
    "        if return_attention:\n",
    "            return result, attention_weights\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 13: Dataset for classification\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, claims, evidence_dict, label_map, embedding_func, embedding_params):\n",
    "       \n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for cid, item in tqdm(claims.items(), desc=\"Building classification dataset\"):\n",
    "            if \"claim_label\" in item and \"evidences\" in item and \"claim_text\" in item:\n",
    "                claim_text = item[\"claim_text\"]\n",
    "                evidence_ids = item[\"evidences\"]\n",
    "                label = item[\"claim_label\"]\n",
    "                \n",
    "                # Get evidence texts\n",
    "                evidence_texts = []\n",
    "                for eid in evidence_ids:\n",
    "                    if eid in evidence_dict:\n",
    "                        evidence_texts.append(evidence_dict[eid])\n",
    "                \n",
    "                if evidence_texts:\n",
    "                    # Concatenate all evidence for this claim\n",
    "                    combined_evidence = \" \".join(evidence_texts[:5])  # Limit to first 5 for memory\n",
    "                    \n",
    "                    # Save sample and label\n",
    "                    self.samples.append((claim_text, combined_evidence))\n",
    "                    self.labels.append(label_map[label])\n",
    "        \n",
    "        self.embedding_func = embedding_func\n",
    "        self.embedding_params = embedding_params\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        claim_text, evidence_text = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Get embedding using the provided function\n",
    "        embedding = self.embedding_func(claim_text, evidence_text, **self.embedding_params)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return embedding_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 14: Training function for classifier\n",
    "def train_classifier(model, train_loader, val_loader, num_epochs, device, patience=3):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    # Use NLLLoss since our model outputs log-softmax probabilities\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(batch_x)  # Model now outputs log-softmax probabilities\n",
    "            loss = criterion(log_probs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Training metrics - still use argmax since highest log probability corresponds to highest probability\n",
    "            preds = torch.argmax(log_probs, dim=1)\n",
    "            total_correct += (preds == batch_y).sum().item()\n",
    "            total_count += len(batch_y)\n",
    "            total_loss += loss.item() * len(batch_y)\n",
    "        \n",
    "        train_loss = total_loss / total_count\n",
    "        train_acc = total_correct / total_count\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_total_correct = 0\n",
    "        val_total_count = 0\n",
    "        val_total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in tqdm(val_loader, desc=\"Validation\"):\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                log_probs = model(batch_x)\n",
    "                loss = criterion(log_probs, batch_y)\n",
    "                preds = torch.argmax(log_probs, dim=1)\n",
    "                val_total_correct += (preds == batch_y).sum().item()\n",
    "                val_total_count += len(batch_y)\n",
    "                val_total_loss += loss.item() * len(batch_y)\n",
    "        \n",
    "        val_loss = val_total_loss / val_total_count\n",
    "        val_acc = val_total_correct / val_total_count\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_classifier.pth')\n",
    "            print(\"  Saved new best model\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement for {patience_counter} epochs\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_classifier.pth'))\n",
    "    return model, best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 15: Main training pipeline\n",
    "def main_training_pipeline(top_k=5):\n",
    "    \n",
    "    LABEL_MAP = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT_ENOUGH_INFO\": 2, \"DISPUTED\": 3}\n",
    "    ID2LABEL = {v: k for k, v in LABEL_MAP.items()}\n",
    "    \n",
    "    # Decide which embedding method to use based on Word2Vec availability\n",
    "    if use_tfidf_fallback:\n",
    "        print(\"Using TF-IDF embeddings for training\")\n",
    "        # Create TF-IDF representations for all evidence\n",
    "        evidence_texts = list(evidence_data.values())\n",
    "        evidence_embeddings, svd_model, vectorizer = create_tfidf_embeddings(\n",
    "            evidence_texts, n_components=256\n",
    "        )\n",
    "        \n",
    "        # Use TF-IDF embedding functions\n",
    "        retrieve_func = lambda claim, ev_dict, k: retrieve_top_k_evidence_tfidf(\n",
    "            claim, ev_dict, vectorizer, svd_model, evidence_embeddings, k\n",
    "        )\n",
    "        \n",
    "        embedding_func = get_tfidf_combined_embedding\n",
    "        embedding_params = {\"vectorizer\": vectorizer, \"svd_model\": svd_model}\n",
    "    else:\n",
    "        print(\"Using Word2Vec embeddings for training\")\n",
    "        # Create evidence embeddings using Word2Vec\n",
    "        evidence_texts = list(evidence_data.values())\n",
    "        evidence_embeddings, svd_model = create_embeddings_with_svd(\n",
    "            evidence_texts, word2vec, n_components=256\n",
    "        )\n",
    "        \n",
    "        # Use Word2Vec embedding functions\n",
    "        retrieve_func = lambda claim, ev_dict, k: retrieve_top_k_evidence_word2vec(\n",
    "            claim, ev_dict, word2vec, svd_model, evidence_embeddings, k\n",
    "        )\n",
    "        \n",
    "        embedding_func = get_word2vec_combined_embedding\n",
    "        embedding_params = {\"word2vec_model\": word2vec, \"svd_model\": svd_model}\n",
    "    \n",
    "    # Prepare re-ranking data\n",
    "    print(\"Building re-ranking training data...\")\n",
    "    rerank_pairs, rerank_labels = build_reranking_data(train_data, evidence_data)\n",
    "    \n",
    "    # Create re-ranking dataset\n",
    "    rerank_dataset = RerankingDataset(rerank_pairs, rerank_labels, embedding_func, embedding_params)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_size = int(0.8 * len(rerank_dataset))\n",
    "    val_size = len(rerank_dataset) - train_size\n",
    "    train_set, val_set = random_split(rerank_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Data loaders for re-ranker\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32)\n",
    "    \n",
    "    # Initialize re-ranking model\n",
    "    reranker = TransformerReranker(\n",
    "        input_dim=256,  # SVD reduced dimension\n",
    "        hidden_dim=256,\n",
    "        num_heads=8,\n",
    "        num_layers=3\n",
    "    )\n",
    "    \n",
    "    # Train re-ranker\n",
    "    print(f\"Training re-ranker on {device}\")\n",
    "    reranker, best_reranker_acc = train_reranker(\n",
    "        reranker, train_loader, val_loader, num_epochs=5, device=device\n",
    "    )\n",
    "    print(f\"Best re-ranker validation accuracy: {best_reranker_acc:.3f}\")\n",
    "    \n",
    "    # Generate enhanced evidence for all training data\n",
    "    print(\"Retrieving and re-ranking evidence for training set...\")\n",
    "    train_data_with_retrieved = {}\n",
    "    \n",
    "    for claim_id, item in tqdm(train_data.items()):\n",
    "        if \"claim_text\" not in item:\n",
    "            continue\n",
    "            \n",
    "        claim_text = item[\"claim_text\"]\n",
    "        \n",
    "        # Step 1: Retrieve initial evidence\n",
    "        initial_evidence = retrieve_func(claim_text, evidence_data, top_k*2)\n",
    "        \n",
    "        # Step 2: Re-rank with transformer\n",
    "        reranked_evidence = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for eid, initial_score in initial_evidence:\n",
    "                evidence_text = evidence_data[eid]\n",
    "                \n",
    "                # Create input for re-ranker\n",
    "                embedding = embedding_func(claim_text, evidence_text, **embedding_params)\n",
    "                input_tensor = torch.tensor(embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Get re-ranking score\n",
    "                relevance_score = reranker(input_tensor, task='ranking').item()\n",
    "                \n",
    "                # Combine initial similarity and re-ranking score\n",
    "                final_score = initial_score * 0.3 + relevance_score * 0.7\n",
    "                reranked_evidence.append((eid, final_score))\n",
    "        \n",
    "        # Sort and take top-k\n",
    "        reranked_evidence.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_evidence_ids = [eid for eid, _ in reranked_evidence[:top_k]]\n",
    "        \n",
    "        # Create enhanced item with retrieved evidence\n",
    "        train_data_with_retrieved[claim_id] = {\n",
    "            \"claim_text\": claim_text,\n",
    "            \"claim_label\": item.get(\"claim_label\", \"\"),\n",
    "            \"evidences\": top_evidence_ids  # Use retrieved evidence\n",
    "        }\n",
    "    \n",
    "    # Prepare classification dataset\n",
    "    print(\"Building classification dataset...\")\n",
    "    classification_dataset = ClassificationDataset(\n",
    "        train_data_with_retrieved, evidence_data, LABEL_MAP, embedding_func, embedding_params\n",
    "    )\n",
    "    \n",
    "    # Split classification dataset\n",
    "    train_size = int(0.8 * len(classification_dataset))\n",
    "    val_size = len(classification_dataset) - train_size\n",
    "    train_clf_set, val_clf_set = random_split(classification_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Data loaders for classifier\n",
    "    train_clf_loader = DataLoader(train_clf_set, batch_size=16, shuffle=True)\n",
    "    val_clf_loader = DataLoader(val_clf_set, batch_size=16)\n",
    "    \n",
    "    # Initialize classifier\n",
    "    classifier = ClaimClassifier(\n",
    "        input_dim=256,  # SVD reduced dimension\n",
    "        hidden_dim=256\n",
    "    )\n",
    "    \n",
    "    # Train classifier\n",
    "    print(f\"Training classifier on {device}\")\n",
    "    classifier, best_clf_acc = train_classifier(\n",
    "        classifier, train_clf_loader, val_clf_loader, num_epochs=5, device=device\n",
    "    )\n",
    "    print(f\"Best classifier validation accuracy: {best_clf_acc:.3f}\")\n",
    "    \n",
    "    return reranker, classifier, retrieve_func, embedding_func, embedding_params, ID2LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 16: Evaluation with re-ranking and classification\n",
    "def evaluate(reranker, classifier, retrieve_func, embedding_func, embedding_params, \n",
    "           id2label, data, evidence_data, device, top_k=5):\n",
    "    \n",
    "    reranker.eval()\n",
    "    classifier.eval()\n",
    "    reranker.to(device)\n",
    "    classifier.to(device)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for claim_id, item in tqdm(data.items(), desc=\"Evaluating\"):\n",
    "        if \"claim_text\" not in item:\n",
    "            continue\n",
    "            \n",
    "        claim_text = item[\"claim_text\"]\n",
    "        \n",
    "        # Step 1: Retrieve initial evidence with similarities\n",
    "        initial_evidence = retrieve_func(claim_text, evidence_data, top_k*2)\n",
    "        \n",
    "        # Step 2: Re-rank evidence using transformer\n",
    "        reranked_evidence = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for eid, initial_score in initial_evidence:\n",
    "                evidence_text = evidence_data[eid]\n",
    "                \n",
    "                # Create input for re-ranker\n",
    "                embedding = embedding_func(claim_text, evidence_text, **embedding_params)\n",
    "                input_tensor = torch.tensor(embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Get re-ranking score (convert from log space to probability)\n",
    "                log_relevance_score = reranker(input_tensor, task='ranking').item()\n",
    "                relevance_score = np.exp(log_relevance_score)  # Convert log probability to probability\n",
    "                \n",
    "                # Combine initial similarity and re-ranking score\n",
    "                final_score = initial_score * 0.3 + relevance_score * 0.7\n",
    "                reranked_evidence.append((eid, final_score))\n",
    "        \n",
    "        # Sort by final score and take top-k\n",
    "        reranked_evidence.sort(key=lambda x: x[1], reverse=True)\n",
    "        final_evidence_ids = [eid for eid, _ in reranked_evidence[:top_k]]\n",
    "        \n",
    "        # Step 3: Classification using top-k evidence\n",
    "        evidence_texts = [evidence_data[eid] for eid in final_evidence_ids]\n",
    "        combined_evidence = \" \".join(evidence_texts)\n",
    "        \n",
    "        # Create input for classifier\n",
    "        embedding = embedding_func(claim_text, combined_evidence, **embedding_params)\n",
    "        input_tensor = torch.tensor(embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Classify using log probabilities\n",
    "        with torch.no_grad():\n",
    "            log_probs = classifier(input_tensor)  # Model outputs log-softmax values\n",
    "            pred_idx = torch.argmax(log_probs, dim=1).item()  # argmax works the same on log probs\n",
    "            pred_label = id2label[pred_idx]\n",
    "        \n",
    "        results[claim_id] = {\n",
    "            \"claim_label\": pred_label,\n",
    "            \"evidences\": final_evidence_ids\n",
    "        }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 17: Save predictions and evaluate\n",
    "def save_and_evaluate(predictions, output_path, groundtruth_path=None):\n",
    "    \n",
    "    # Save predictions\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    \n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "    \n",
    "    # Run evaluation if groundtruth is provided\n",
    "    if groundtruth_path and os.path.exists(groundtruth_path):\n",
    "        try:\n",
    "            # Try to import and run eval directly\n",
    "            import sys\n",
    "            sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "            \n",
    "            try:\n",
    "                from eval import main as eval_main\n",
    "                \n",
    "                class Args:\n",
    "                    def __init__(self):\n",
    "                        self.predictions = output_path\n",
    "                        self.groundtruth = groundtruth_path\n",
    "                        self.verbose = False\n",
    "                \n",
    "                print(\"\\nRunning evaluation...\")\n",
    "                eval_main(Args())\n",
    "            except ImportError:\n",
    "                # Fallback to subprocess\n",
    "                import subprocess\n",
    "                cmd = f\"python eval.py --predictions {output_path} --groundtruth {groundtruth_path}\"\n",
    "                \n",
    "                result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "                print(\"\\nEvaluation Results:\")\n",
    "                print(result.stdout)\n",
    "        except Exception as e:\n",
    "            print(f\"Error running evaluation: {e}\")\n",
    "            print(f\"Please run manually: python eval.py --predictions {output_path} --groundtruth {groundtruth_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 18: Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Run full pipeline with different top-k values\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING MAIN TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Train models\n",
    "    reranker, classifier, retrieve_func, embedding_func, embedding_params, id2label = main_training_pipeline()\n",
    "    \n",
    "    # Evaluate on development set with top-k=3\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATING WITH TOP-K=3\")\n",
    "    print(\"=\"*80)\n",
    "    dev_predictions_top4 = evaluate(reranker, classifier, retrieve_func, embedding_func, \n",
    "                                   embedding_params, id2label, dev_data, evidence_data, device, top_k=3)\n",
    "    save_and_evaluate(dev_predictions_top4, \"dev-claims-predictions-top4.json\", \"data/dev-claims.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(54979) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence Retrieval F-score (F)    = 0.0733353947639662\n",
      "Claim Classification Accuracy (A) = 0.44155844155844154\n",
      "Harmonic Mean of F and A          = 0.1257807351291487\n"
     ]
    }
   ],
   "source": [
    "!python eval.py --predictions dev-claims-predictions-top4.json --groundtruth data/dev-claims.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
